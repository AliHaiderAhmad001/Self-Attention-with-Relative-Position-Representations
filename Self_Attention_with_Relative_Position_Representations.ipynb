{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Self-Attention with Relative Position Representations**\n",
        "\n",
        "**Note:** I advise you to watch [this video](https://www.youtube.com/watch?v=DwaBQbqh5aE) if you do not have prior knowledge of this type of representation.\n",
        "\n",
        "The self-attention mechanism in original Transformer is extended to efficiently consider representations of the relative positions, or distances between sequence elements. Abstract of the original paper:\n",
        "\n",
        "> Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.\n",
        "\n",
        "### **Self-Attention**\n",
        "\n",
        "1. **Input and Output Dimensions:**\n",
        "   - Input sequence $x=\\left(x_{1}, \\ldots, x_{n}\\right)$ of $n$ elements with dimension $d_a$.\n",
        "   - Output sequence $z=\\left(z_{1}, \\ldots, z_{n}\\right)$ with dimension $d_z$.\n",
        "\n",
        "2. **Computation of $z_i$:**\n",
        "   $$\n",
        "z_{i}=\\sum_{j=1}^{n} \\alpha_{i j}\\left(x_{j} W^{V}\\right)\n",
        "$$\n",
        "\n",
        "3. **Weight Coefficients $\\alpha_{ij}\\$:**\n",
        "   $$\n",
        "\\alpha_{i j}=\\frac{\\exp e_{i j}}{\\sum_{k=1}^{n} \\exp e_{i k}}\n",
        "$$\n",
        "\n",
        "4. **Compatibility Function $e_{ij}$:**\n",
        "$$\n",
        "e_{i j}=\\frac{\\left(x_{i} W^{Q}\\right)\\left(x_{j} W^{K}\\right)^{T}}{\\sqrt{d_{z}}}\n",
        "$$\n",
        "\n",
        "### **Relation-aware Self-Attention**\n",
        "\n",
        "1. **Extension to Self-Attention:**\n",
        "   - An extension to self-attention is proposed to consider the pairwise relationships between input elements in the sense that the input is modeled as a labeled, directed, fully-connected graph. The edge between input elements $x_i$ and $x_j$ is represented by vectors: $a_{i j}^{V}, a_{i j}^{K} \\in \\mathbb{R}^{d_{a}}$. So, $a_{i j}^{V}, a_{i j}^{K}$ model the interaction between positions $i$ and $j$. These representations can be shared across attention heads. We use $d_a = d_z$ .\n",
        "   - These representations can be shared across attention heads.\n",
        "   - Edges can capture information about the relative position differences between input elements.\n",
        "\n",
        "2. **Modification of $z_i$ with Edge Information $a_{ij}^V$:**\n",
        "   $$\n",
        "z_{i}=\\sum_{j=1}^{n} \\alpha_{i j}\\left(x_{j} W^{V}+a_{i j}^{V}\\right)\n",
        "$$\n",
        "3. **Modification of Compatibility Function with Edge Information $a_{ij}^K$:**\n",
        "$$\n",
        "e_{i j}=\\frac{x_{i} W^{Q}\\left(x_{j} W^{K}+a_{i j}^{K}\\right)^{T}}{\\sqrt{d_{z}}}\n",
        "$$\n",
        "\n",
        "### **Relative Position Representations**\n",
        "\n",
        "1. **Edge Labels for Relative Positions:**\n",
        "   - Edge representations $a_{ij}^K, a_{ij}^V$ capture relative position differences.\n",
        "   - The maximum relative position is clipped to a maximum absolute value of $k$. It is hypothesized that precise relative position information is not useful beyond a certain distance. Clipping the maximum distance also enables the model to generalize to sequence lengths not seen during training.\n",
        "   - So, $2k+1$ unique edge labels are considered.\n",
        "\n",
        "2. **Learnable Relative Position Representations:**\n",
        "   - $a_{ij}^K, a_{ij}^V$ are determined using learnable relative position representations $w^K, w^V$.\n",
        "   - Clipping function:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "a_{i j}^{K} & =w_{\\operatorname{clip}(j-i, k)}^{K} \\\\\n",
        "a_{i j}^{V} & =w_{\\operatorname{clip}(j-i, k)}^{V} \\\\\n",
        "\\operatorname{clip}(x, k) & =\\max (-k, \\min (k, x))\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "3. **Learnable Vectors \\( w^K, w^V \\):**\n",
        "   - $w^K = (w_{-k}^K, \\ldots, w_k^K)$\n",
        "   - $w^V = (w_{-k}^V, \\ldots, w_k^V)$\n",
        "\n",
        "### **Analysis**\n",
        "\n",
        "* Among the first, Shaw, Uszkoreit, and Vaswani (2018) introduced an alternative method for incorporating both absolute and relative position encodings.\n",
        "* The use of relative position representations allows the model to consider pairwise relationships and capture information about the relative position differences between input elements, enhancing its ability to understand the sequence structure.\n",
        "* Although it cannot directly be compared with the effect of simple addition of position embeddings, they roughly omit the position–position interaction and have only one unit–position term. In addition, they do not share the projection matrices but directly model the pairwise position interaction with the vectors $a$. In an ablation analysis they found that solely adding $a_{i j}^{K}$ might be sufficient.\n",
        "*  To reduce space complexity, they share the parameters across attention heads. While it is not explicitly mentioned in their paper we understand that they add the position information in each layer but do not share the parameters. The authors find that relative position embeddings perform better in machine translation and the combination of absolute and relative embeddings does not improve the performance.\n",
        "\n",
        "[Ref.1](https://direct.mit.edu/coli/article/48/3/733/111478/Position-Information-in-Transformers-An-Overview), [Ref.2](https://doi.org/10.18653/v1/N18-2074), [Ref.3](https://sh-tsang.medium.com/review-self-attention-with-relative-position-representations-266ab2f78dd7)."
      ],
      "metadata": {
        "id": "S-ILocDiK8K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import List\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class RelativePosition(nn.Module):\n",
        "    \"\"\"\n",
        "    Relative Position Embeddings Module\n",
        "\n",
        "    This module generates learnable relative position embeddings to enrich\n",
        "    the self-attention mechanism with information about the relative distances\n",
        "    between elements in input sequences.\n",
        "\n",
        "    Args:\n",
        "        d_a (int): Number of dimensions in the relative position embeddings.\n",
        "        k (int): Clipping distance.\n",
        "\n",
        "    Attributes:\n",
        "        position_embeddings (nn.Parameter): Learnable parameter for relative position embeddings.\n",
        "\n",
        "    Example:\n",
        "        >>> # Create a RelativePosition instance with 16 dimensions and clipping distance of 10\n",
        "        >>> relative_position = RelativePosition(d_a=16, k=10)\n",
        "        >>> # Generate relative position embeddings for sequences of lengths 5 and 7\n",
        "        >>> embeddings = relative_position(length_query=5, length_key=7)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_a: int, k: int):\n",
        "        \"\"\"\n",
        "        Initialize the RelativePosition module.\n",
        "\n",
        "        Args:\n",
        "        - d_a (int): Number of dimensions in the relative position embeddings.\n",
        "        - k (int): Clipping distance.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.d_a = d_a\n",
        "        self.k = k\n",
        "        self.position_embeddings = nn.Parameter(torch.empty((2 * k + 1, d_a)))\n",
        "        nn.init.xavier_uniform_(self.position_embeddings)\n",
        "\n",
        "    def forward(self, length_query: int, length_key: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute relative position embeddings.\n",
        "\n",
        "        Args:\n",
        "        - length_query (int): Length of the query sequence.\n",
        "        - length_key (int): Length of the key sequence.\n",
        "\n",
        "        Returns:\n",
        "        - embeddings (torch.Tensor): Relative position embeddings (length_query, length_key, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Generate relative position embeddings\n",
        "        indices_query = torch.arange(length_query, device=self.position_embeddings.device)\n",
        "        indices_key = torch.arange(length_key, device=self.position_embeddings.device)\n",
        "        distance_matrix = indices_key.unsqueeze(0) - indices_query.unsqueeze(1)\n",
        "        distance_matrix_clipped = torch.clamp(distance_matrix, -self.k, self.k)\n",
        "        final_matrix = distance_matrix_clipped + self.k\n",
        "        embeddings = self.position_embeddings[final_matrix.to(torch.long)]\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class RelationAwareAttentionHead(nn.Module):\n",
        "    \"\"\"\n",
        "    Relation-aware attention head implementation.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        head_dim (int): Dimensionality of the attention head.\n",
        "        k_bias_matrix (torch.Tensor): Matrix for relative position attention in query-key interaction.\n",
        "        v_bias_matrix (torch.Tensor): Matrix for relative position attention in query-value interaction.\n",
        "\n",
        "    Attributes:\n",
        "        query_weights (nn.Linear): Linear layer for query projection.\n",
        "        key_weights (nn.Linear): Linear layer for key projection.\n",
        "        value_weights (nn.Linear): Linear layer for value projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, head_dim, k_bias_matrix, v_bias_matrix):\n",
        "        \"\"\"\n",
        "        Initializes the RelationAwareAttentionHead.\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "            head_dim (int): Dimensionality of the attention head.\n",
        "            k_bias_matrix (torch.Tensor): Matrix for relative position attention in query-key interaction.\n",
        "            v_bias_matrix (torch.Tensor): Matrix for relative position attention in query-value interaction.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "        self.query_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.key_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.value_weights: nn.Linear = nn.Linear(hidden_size, head_dim)\n",
        "        self.k_bias_matrix = k_bias_matrix\n",
        "        self.v_bias_matrix = v_bias_matrix\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor.\n",
        "            key (torch.Tensor): Key tensor.\n",
        "            value (torch.Tensor): Value tensor.\n",
        "            mask (torch.Tensor): Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated value embeddings after applying attention mechanism.\n",
        "        \"\"\"\n",
        "        query: torch.Tensor = self.query_weights(query) # (b_s, n_t, head_dim)\n",
        "        key: torch.Tensor = self.key_weights(key) # (b_s, n_t, head_dim)\n",
        "        value: torch.Tensor = self.value_weights(value) # (b_s, n_t, head_dim)\n",
        "\n",
        "        # Self-Attention scores\n",
        "        attn_1: torch.Tensor = torch.matmul(query, key.transpose(1, 2)) # Q*K^T:(b_s, n_t, n_t)\n",
        "\n",
        "        # Relative Position Attention scores\n",
        "        attn_2: torch.Tensor = torch.matmul(query.permute(1, 0, 2), self.k_bias_matrix.transpose(1, 2)).transpose(0, 1) # Q*K_shifting^T:(b_s, n_t, n_t)\n",
        "\n",
        "        # Relation-aware Self-Attention scores\n",
        "        att_scores: torch.Tensor = (attn_1 + attn_2)/self.head_dim ** 0.5\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.to(torch.int)\n",
        "            att_scores: torch.Tensor = att_scores.masked_fill(mask.unsqueeze(1) == 0, -1e9)\n",
        "\n",
        "        att_weights: torch.Tensor = F.softmax(att_scores, dim=-1)\n",
        "\n",
        "        # Weighted sum of values\n",
        "        values_1: torch.Tensor = torch.matmul(att_weights, value) # (b_s, n_t, head_dim)\n",
        "\n",
        "        # Relative Position Representation for values\n",
        "        values_2: torch.Tensor = torch.matmul(att_weights.permute(1, 0, 2), self.v_bias_matrix).transpose(0, 1) # (b_s, n_t, head_dim)\n",
        "\n",
        "        # Relation-aware values\n",
        "        n_value  = values_1 + values_2\n",
        "        return n_value\n",
        "\n",
        "\n",
        "class RelationAwareMultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention layer implementation.\n",
        "\n",
        "    Args:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        num_heads (int): Number of attention heads.\n",
        "        k (int): Clipping distance for relative position embeddings.\n",
        "        seq_len (int): Length of the input sequences.\n",
        "\n",
        "    Attributes:\n",
        "        hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "        num_heads (int): Number of attention heads.\n",
        "        head_dim (int): Dimensionality of each attention head.\n",
        "        relative_position_k (RelativePosition): Instance of RelativePosition for query-key relative positions.\n",
        "        relative_position_v (RelativePosition): Instance of RelativePosition for query-value relative positions.\n",
        "        k_bias_matrix (torch.Tensor): Matrix for relative position attention in query-key interaction.\n",
        "        v_bias_matrix (torch.Tensor): Matrix for relative position attention in query-value interaction.\n",
        "        attention_heads (nn.ModuleList): List of RelationAwareAttentionHead layers.\n",
        "        fc (nn.Linear): Fully connected layer for final projection.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, num_heads, k, seq_len):\n",
        "        \"\"\"\n",
        "        Initializes the RelationAwareMultiHeadAttention layer.\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Hidden size for the model (embedding dimension).\n",
        "            num_heads (int): Number of attention heads.\n",
        "            k (int): Clipping distance for relative position embeddings.\n",
        "            seq_len (int): Length of the input sequences.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size: int = hidden_size\n",
        "        self.num_heads: int = num_heads\n",
        "        self.head_dim: int = hidden_size // num_heads\n",
        "        self.relative_position_k: torch.Tensor = RelativePosition(self.head_dim, k)\n",
        "        self.relative_position_v: torch.Tensor = RelativePosition(self.head_dim, k)\n",
        "        self.k_bias_matrix: torch.Tensor = self.relative_position_k(seq_len, seq_len)\n",
        "        self.v_bias_matrix: torch.Tensor = self.relative_position_v(seq_len, seq_len)\n",
        "        self.attention_heads: nn.ModuleList = nn.ModuleList([RelationAwareAttentionHead(self.hidden_size, self.head_dim,\n",
        "                                                                           self.k_bias_matrix, self.v_bias_matrix) for _ in range(self.num_heads)])\n",
        "        self.fc: nn.Linear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Applies multi-head attention mechanism to the input query, key, and value tensors.\n",
        "\n",
        "        Args:\n",
        "            query (torch.Tensor): Query tensor.\n",
        "            key (torch.Tensor): Key tensor.\n",
        "            value (torch.Tensor): Value tensor.\n",
        "            mask (torch.Tensor): Optional mask tensor.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Updated hidden state after applying multi-head attention mechanism.\n",
        "        \"\"\"\n",
        "        attention_outputs: List[torch.Tensor] = [attention_head(query, key, value, mask=mask) for attention_head in self.attention_heads]\n",
        "        hidden_state: torch.Tensor = torch.cat(attention_outputs, dim=-1)\n",
        "        hidden_state: torch.Tensor = self.fc(hidden_state)\n",
        "        return hidden_state"
      ],
      "metadata": {
        "id": "20jBafCxpzUb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 768\n",
        "num_heads = 8\n",
        "head_dim = 64\n",
        "k = 4\n",
        "seq_len=20\n",
        "batch_size = 16\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Create instances of RelativePosition, RelationAwareAttentionHead, and RelationAwareMultiHeadAttention\n",
        "relative_position_k = RelativePosition(d_a=head_dim, k=k)\n",
        "relative_position_v = RelativePosition(d_a=head_dim, k=k)\n",
        "\n",
        "attention_head = RelationAwareAttentionHead(hidden_size=hidden_size,\n",
        "                                           head_dim=head_dim,\n",
        "                                           k_bias_matrix=relative_position_k(seq_len, seq_len),\n",
        "                                           v_bias_matrix=relative_position_v(seq_len, seq_len))\n",
        "\n",
        "multihead_attention = RelationAwareMultiHeadAttention(hidden_size, num_heads, k, seq_len)\n",
        "\n",
        "# Generate dummy input tensors\n",
        "x_input = torch.rand((batch_size, seq_len, hidden_size))\n",
        "\n",
        "# Test RelativePosition\n",
        "relative_position_embeddings = relative_position_k(seq_len, seq_len)\n",
        "print(\"Relative Position Embeddings Shape:\", relative_position_embeddings.shape)\n",
        "\n",
        "# Test RelationAwareAttentionHead\n",
        "output_attention_head = attention_head(x_input, x_input, x_input)\n",
        "print(\"RelationAwareAttentionHead Output Shape:\", output_attention_head.shape)\n",
        "\n",
        "# Test RelationAwareMultiHeadAttention\n",
        "output_multihead_attention = multihead_attention(x_input, x_input, x_input)\n",
        "print(\"RelationAwareMultiHeadAttention Output Shape:\", output_multihead_attention.shape)\n"
      ],
      "metadata": {
        "id": "j5Ak8qOEyaRb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f820846f-9184-43e5-ad73-bbe5f3c6493c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relative Position Embeddings Shape: torch.Size([20, 20, 64])\n",
            "RelationAwareAttentionHead Output Shape: torch.Size([16, 20, 64])\n",
            "RelationAwareMultiHeadAttention Output Shape: torch.Size([16, 20, 768])\n"
          ]
        }
      ]
    }
  ]
}